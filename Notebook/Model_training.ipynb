{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from cmath import nan\n",
    "from geopandas import GeoDataFrame\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from numpy import array, split, nan\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess data\n",
    "def load_data(file_path, index_col='Date', scaler=None):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data.set_index(index_col)\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        data_scaled = scaler.fit_transform(data.iloc[:-366])\n",
    "    else:\n",
    "        data_scaled = scaler.transform(data)\n",
    "    return data, data_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into sequences\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "  X,y,x_test,y_test=[],[],[],[]\n",
    "  for i in range(len(sequence)):\n",
    "    end_ix=i+n_steps_in\n",
    "    out_end_ix=end_ix+n_steps_out\n",
    "    if out_end_ix>len(sequence):\n",
    "      break\n",
    "    seq_x,seq_y=sequence[i:end_ix],sequence[end_ix:out_end_ix]\n",
    "    if i <= len(sequence)*.8:\n",
    "       X.append(seq_x)\n",
    "       y.append(seq_y)\n",
    "    else:\n",
    "       x_test.append(seq_x)\n",
    "       y_test.append(seq_y)\n",
    "  return np.array(X),np.array(y),np.array(x_test),np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and train LSTM model\n",
    "def build_train_lstm(x_train, y_train, n_features, n_steps_out, n_neurons=100, n_layers=1, n_epoch=100):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, activation='relu', input_shape=(x_train.shape[1], n_features)))\n",
    "    model.add(RepeatVector(n_steps_out))\n",
    "    for _ in range(n_layers):\n",
    "        model.add(LSTM(n_neurons, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(n_features)))\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    history = model.fit(x_train, y_train, epochs=n_epoch, validation_split=0.2, batch_size=64, verbose=0)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model and generate predictions\n",
    "def evaluate_model(model, x_test, y_test, scaler):\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_unscaled = scaler.inverse_transform(y_pred.reshape(-1, x_test.shape[-1]))\n",
    "    y_test_unscaled = scaler.inverse_transform(y_test.reshape(-1, x_test.shape[-1]))\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_unscaled, y_pred_unscaled))\n",
    "    mae = mean_absolute_error(y_test_unscaled, y_pred_unscaled)\n",
    "    return rmse, mae, y_pred_unscaled, y_test_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot learning curve\n",
    "def plot_learning_curve(history):\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Curve for LSTM')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_formating (M_data,n_steps_out,y_test_un,y_pred):    \n",
    "    COL= M_data.columns\n",
    "    results = pd.DataFrame()\n",
    "    results['Stations_Feature'] = COL\n",
    "    for id_cols in range(1,n_steps_out+1,1): \n",
    "        results['Actual '+'Day ' +str(id_cols)] = nan\n",
    "        results['Predicted '+ 'Day '+str(id_cols)] = nan\n",
    "    for ith in range(0,n_steps_out,1):\n",
    "        ids = ith + 1\n",
    "        for idx in range(len(results)):\n",
    "            results.iloc[idx,results.columns.get_loc('Actual '+'Day '+str(ids))] = y_test_un[0][idx] \n",
    "            results.iloc[idx,results.columns.get_loc('Predicted '+'Day '+str(ids))]= y_pred[0][idx]\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    file_path = '..\\Datasets\\Input_data\\Weather_Data.csv'\n",
    "    M_data, ET_Data, scaler = load_data(file_path)\n",
    "    \n",
    "    # Define parameters\n",
    "    n_steps_in, n_steps_out = 7, 1\n",
    "    n_features = M_data.shape[1]\n",
    "    \n",
    "    # Split data into sequences\n",
    "    X, y,xt,yt = split_sequence(ET_Data, n_steps_in, n_steps_out)\n",
    "    \n",
    "    # Reshape data for LSTM\n",
    "    x_train = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "    y_train = y.reshape((y.shape[0], y.shape[1], n_features))\n",
    "    x_test = xt.reshape((xt.shape[0], xt.shape[1], n_features))\n",
    "    \n",
    "    # Build and train LSTM model\n",
    "    model, history = build_train_lstm(x_train, y_train, n_features, n_steps_out,n_steps_in)\n",
    "    \n",
    "    # Plot learning curve\n",
    "    plot_learning_curve(history)\n",
    "    \n",
    "    # # Generate predictions and evaluate model\n",
    "    rmse, mae, y_pred, y_test_un = evaluate_model(model, x_test, yt, scaler)\n",
    "\n",
    "    print(f'Total RMSE: {rmse}, MAE: {mae}')\n",
    "    results = data_formating (M_data,n_steps_out,y_test_un,y_pred)\n",
    "    \n",
    "    # Save model\n",
    "    model.save('Days_ahead_pred.keras')\n",
    "    \n",
    "    # Export predictions\n",
    "    results.to_csv(r'..\\Datasets\\Output_data\\Prediction.csv', index=False)\n",
    "    from plotting import plot_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "os_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
